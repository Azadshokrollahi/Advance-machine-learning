{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language translation with Torchtext!! ## \n",
    "\n",
    "Seq2Seq network with torchtext\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torchtext\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import math\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alvar\\projects\\python\\advanced-ml-labs\\2-language-models-lab\\.data\\train.de.gz: 100%|█| 637k/637k [00:00<00:00\n",
      "C:\\Users\\alvar\\projects\\python\\advanced-ml-labs\\2-language-models-lab\\.data\\train.en.gz: 100%|█| 569k/569k [00:00<00:00\n",
      "C:\\Users\\alvar\\projects\\python\\advanced-ml-labs\\2-language-models-lab\\.data\\val.de.gz: 100%|█| 24.7k/24.7k [00:00<00:00\n",
      "C:\\Users\\alvar\\projects\\python\\advanced-ml-labs\\2-language-models-lab\\.data\\val.en.gz: 100%|█| 21.6k/21.6k [00:00<00:00\n",
      "C:\\Users\\alvar\\projects\\python\\advanced-ml-labs\\2-language-models-lab\\.data\\test_2016_flickr.de.gz: 100%|█| 22.9k/22.9k\n",
      "C:\\Users\\alvar\\projects\\python\\advanced-ml-labs\\2-language-models-lab\\.data\\test_2016_flickr.en.gz: 100%|█| 21.1k/21.1k\n",
      "C:\\Anaconda3\\envs\\pDL\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n",
      "C:\\Anaconda3\\envs\\pDL\\lib\\site-packages\\torchtext\\data\\utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "        for string_ in f:\n",
    "            counter.update(tokenizer(string_))\n",
    "    \n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
    "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
    "\n",
    "def data_process(filepaths):\n",
    "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "    src_max_length = 0\n",
    "    tgt_max_length = 0\n",
    "    data = []\n",
    "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n",
    "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n",
    "        data.append((de_tensor_, en_tensor_))\n",
    "        \n",
    "        if de_tensor_.size(0) > src_max_length:\n",
    "            src_max_length = de_tensor_.size(0)\n",
    "            \n",
    "        if en_tensor_.size(0) > tgt_max_length:\n",
    "            tgt_max_length = en_tensor_.size(0)\n",
    "        \n",
    "    return data, src_max_length, tgt_max_length\n",
    "\n",
    "train_data, trsl, trtl = data_process(train_filepaths)\n",
    "val_data, vsl, vtl = data_process(val_filepaths)\n",
    "test_data, tesl, tetl = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "# We calculate the sentence with max_length\n",
    "candidates_lengths = [trsl, trtl, vsl, vtl, tesl, tetl]\n",
    "print(max(candidates_lengths))\n",
    "MAX_LENGTH = max(candidates_lengths) + 2\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    de_batch, en_batch, en_batch_out = [], [], []\n",
    "    a = 0\n",
    "    for (de_item, en_item) in data_batch:\n",
    "        de_extra = MAX_LENGTH - (de_item.size(0) + 2)\n",
    "        en_extra = MAX_LENGTH - (en_item.size(0) + 2)\n",
    "        en_inp_extra = MAX_LENGTH - (en_item.size(0) + 1)\n",
    "        \n",
    "        de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX]), torch.full((de_extra,), PAD_IDX)], dim=0))\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX]), torch.full((en_extra,), PAD_IDX)], dim=0))\n",
    "        en_batch_out.append(torch.cat([en_item, torch.tensor([EOS_IDX]), torch.full((en_inp_extra,), PAD_IDX)], dim=0)) # Target input \n",
    "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "    en_batch_out = pad_sequence(en_batch_out, padding_value=PAD_IDX)\n",
    "    return de_batch, en_batch, en_batch_out\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_id_to_text = lambda x: [de_vocab.itos[t] for t in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_id_to_text = lambda x: [en_vocab.itos[t] for t in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we create the Seq2Seq model ##\n",
    "\n",
    "The Seq2Seq model is an encoder-decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest seq2seq decoder we use only last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector is used as the initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder’s last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, gru_layers):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.gru_layers = gru_layers\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, gru_layers)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size()[0]\n",
    "#         print(\"X :\", x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder ###\n",
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to “focus” on a different part of the encoder’s outputs for every step of the decoder’s own outputs. First we calculate a set of attention weights. These will be multiplied by the encoder output vectors to create a weighted combination. The result (called attn_applied in the code) should contain information about that specific part of the input sequence, and thus help the decoder choose the right output words.\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward layer attn, using the decoder’s input and hidden state as inputs. Because there are sentences of all sizes in the training data, to actually create and train this layer we have to choose a maximum sentence length (input length, for encoder outputs) that it can apply to. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, gru_layers, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.gru_layers = gru_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, gru_layers)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        batch_size = x.size()[0]\n",
    "        # Embedding: a simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        # This module is often used to store word embeddings and retrieve them using indices. \n",
    "        # The input to the module is a list of indices, and the output is the corresponding word embeddings\n",
    "#         print(\"INPUT SHAPE: \", x.shape)\n",
    "#         print(batch_size)\n",
    "        embedded = self.embedding(x)\n",
    "#         print(\"EMBEDD: \", embedded.shape)\n",
    "        embedded = embedded.view(1, batch_size, -1)\n",
    "#         print(\"EMBEDD VIEW: \", embedded.shape)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        \n",
    "        # torch.bmm performs a batch matrix-matrix product of matrices stored in input and mat2.\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return torch.zeros(self.gru_layers, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We  create the classic train/evaluate loop ###\n",
    "\n",
    "For this task, we have something called **teacher_forcing_ratio**, this helps us vary between giving the network the possibility to try to translate using its own previous prediction (no teacher forcing), or we use the known target for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, target_tensor_out, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "\n",
    "    encoder_hidden = encoder.initHidden(input_tensor.size(1))\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    # We run the input sequence through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "#         print(encoder_output.shape)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "    \n",
    "    decoder_input = torch.full((target_tensor.size(1),), BOS_IDX, device=device)\n",
    "#     decoder_input = torch.tensor([[BOS_IDX], target_tensor.size(1)], device=device)\n",
    "\n",
    "#     print(decoder_input.shape)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoded_words = []\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(english_id_to_text(topi[0]))\n",
    "            loss += criterion(decoder_output, target_tensor_out[di])\n",
    "            if di + 1 < target_length:\n",
    "                decoder_input = target_tensor[di + 1]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoded_words.append(english_id_to_text(topi[0]))\n",
    "            decoder_input = topi.detach()  # detach from history as input\n",
    "#             print(target_tensor[di].shape)\n",
    "            loss += criterion(decoder_output, target_tensor_out[di])\n",
    "\n",
    "#     print(decoded_words)\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        encoder_hidden = encoder.initHidden(input_tensor.size(1))\n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_size, device=device)\n",
    "        \n",
    "        # We run the input sequence through the encoder\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "#         print(encoder_outputs[:1])\n",
    "        decoder_input = torch.full((input_tensor.size(1),), BOS_IDX, device=device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n",
    "\n",
    "        for di in range(MAX_LENGTH):\n",
    "#             print(decoder_input)\n",
    "#             print(decoder_input.shape)\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_IDX:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(english_id_to_text(topi))\n",
    "#             print(decoded_words)\n",
    "            decoder_input = topi.detach()\n",
    "          \n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s' % (asMinutes(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, epochs, print_every=50, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    PAD_IDX = en_vocab.stoi['<pad>']\n",
    "    criterion = nn.NLLLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(0, epochs +1):\n",
    "        print_loss_total = 0  # Reset every print_every\n",
    "        plot_loss_total = 0  # Reset every plot_every\n",
    "        print(\"-----------------------\")\n",
    "        print(\"EPOCH: \", epoch, \" out of \", epochs, \" epochs\")\n",
    "        print(\"-----------------------\")\n",
    "        for batch_counter, (src, trg, trg_output) in enumerate(train_iter):\n",
    "            src, trg, trg_output = src.to(device), trg.to(device), trg_output.to(device)\n",
    "            src_ = src.permute(1,0)\n",
    "            trg_ = trg.permute(1,0)\n",
    "            trg_output_ = trg_output.permute(1, 0)\n",
    "            \n",
    "            # Uncomment to go 1 by 1 manual unbatch version (preferably simply set batch = 1)\n",
    "#             for j in range(len(src_)):\n",
    "#                 loss = train(src_[j].unsqueeze(dim=1), trg_[j].unsqueeze(dim=1), trg_output_[j].unsqueeze(dim=1), encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "#                 print_loss_total += loss\n",
    "#                 plot_loss_total += loss\n",
    "                \n",
    "            loss = train(src, trg, trg_output, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "    #     training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "    #                       for i in range(n_iters)]\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "            if batch_counter % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (timeSince(start, batch_counter / len(train_iter)),\n",
    "                                             batch_counter, batch_counter/ len(train_iter) * 100, print_loss_avg))\n",
    "\n",
    "            if batch_counter % plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,498,048 trainable parameters\n",
      "The model has 7,294,341 trainable parameters\n",
      "-----------------------\n",
      "EPOCH:  0  out of  5  epochs\n",
      "-----------------------\n",
      "0m 17s (50 22%) 4.3052\n",
      "0m 33s (100 44%) 3.3924\n",
      "0m 49s (150 66%) 3.2492\n",
      "1m 5s (200 88%) 3.1109\n",
      "-----------------------\n",
      "EPOCH:  1  out of  5  epochs\n",
      "-----------------------\n",
      "1m 30s (50 22%) 2.8950\n",
      "1m 46s (100 44%) 2.9928\n",
      "2m 3s (150 66%) 2.8951\n",
      "2m 20s (200 88%) 2.7916\n",
      "-----------------------\n",
      "EPOCH:  2  out of  5  epochs\n",
      "-----------------------\n",
      "2m 45s (50 22%) 2.8020\n",
      "3m 1s (100 44%) 2.6477\n",
      "3m 17s (150 66%) 2.7538\n",
      "3m 34s (200 88%) 2.7635\n",
      "-----------------------\n",
      "EPOCH:  3  out of  5  epochs\n",
      "-----------------------\n",
      "3m 59s (50 22%) 2.7211\n",
      "4m 16s (100 44%) 2.9030\n",
      "4m 32s (150 66%) 2.6883\n",
      "4m 49s (200 88%) 2.6083\n",
      "-----------------------\n",
      "EPOCH:  4  out of  5  epochs\n",
      "-----------------------\n",
      "5m 14s (50 22%) 2.8162\n",
      "5m 31s (100 44%) 2.6291\n",
      "5m 47s (150 66%) 2.5408\n",
      "6m 3s (200 88%) 2.7916\n",
      "-----------------------\n",
      "EPOCH:  5  out of  5  epochs\n",
      "-----------------------\n",
      "6m 29s (50 22%) 2.6181\n",
      "6m 47s (100 44%) 2.7091\n",
      "7m 4s (150 66%) 2.6839\n",
      "7m 21s (200 88%) 2.7179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "INPUT_DIM = len(de_vocab)\n",
    "OUTPUT_DIM = len(en_vocab)\n",
    "hidden_size = 256\n",
    "\n",
    "encoder1 = EncoderRNN(INPUT_DIM, hidden_size, 4).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, OUTPUT_DIM, 4, dropout_p=0.1).to(device)\n",
    "\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(encoder1):,} trainable parameters')\n",
    "print(f'The model has {count_parameters(attn_decoder1):,} trainable parameters')\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, epochs = 5, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly evaluate\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for _, (src, trg, tgt_output) in enumerate(test_iter):\n",
    "        for i in range(n):\n",
    "#             print(src.shape)\n",
    "            src_ = src.permute(1,0)\n",
    "            trg_ = trg.permute(1,0)\n",
    "#             print(src_[i])\n",
    "#             print(src_[i].unsqueeze(dim=1).shape)\n",
    "\n",
    "            src_ = src_[i]\n",
    "            trg_ = trg_[i]\n",
    "            print('>', german_id_to_text(src_))\n",
    "            print('=', english_id_to_text(trg_))\n",
    "            output_words, attentions = evaluate(encoder, decoder, src_.unsqueeze(dim=1))\n",
    "#             output_sentence = ' '.join(output_words)\n",
    "            print('<', output_words)\n",
    "            print('')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['<bos>', 'Ein', 'Kind', 'liegt', 'auf', 'dem', 'Boden', 'neben', 'einem', 'Sportwagen', '.', '\\n', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "= ['<bos>', 'A', 'child', 'is', 'laying', 'on', 'the', 'ground', 'next', 'to', 'a', 'stroller', '.', '\\n', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "< [['A'], ['man'], ['in'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['a'], ['.'], ['.'], ['\\n'], ['\\n'], ['\\n'], ['\\n'], '<EOS>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Machine Translation is hard task!! ##\n",
    "\n",
    "The dataset we have + the parameters + training time is not really enough to train a machine translation network! \n",
    "\n",
    "Implement and test Pytorch’s own Transformer models for machine translation\n",
    "Analyze based on the output and measurements.\n",
    "Compare to provided example seq2seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
