{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python native data types\n",
    "\n",
    "Python has many native datatypes. Here are the important ones:\n",
    "\n",
    " - **Booleans** are either `True` or `False`.\n",
    " - **Numbers** can be integers (1 and 2), floats (1.1 and 1.2), fractions (1/2 and 2/3), or even complex numbers.\n",
    " - **Strings** are sequences of Unicode characters, e.g. an html document.\n",
    " - **Lists** are ordered sequences of values.\n",
    " - **Tuples** are ordered, immutable sequences of values.\n",
    " - **Sets** are unordered bags of values.\n",
    " - **Dictionaries** are unordered bags of key-value pairs.\n",
    " \n",
    "See [here](http://www.diveintopython3.net/native-datatypes.html) for a complete overview.\n",
    "\n",
    "### More resources\n",
    "\n",
    " 1. Brief Python introduction [here](https://learnxinyminutes.com/docs/python3/).\n",
    " 2. Full Python tutorial [here](https://docs.python.org/3/tutorial/).\n",
    " 3. A Whirlwind Tour of Python [here](https://github.com/jakevdp/WhirlwindTourOfPython).\n",
    " 4. Python Data Science Handbook [here](https://github.com/jakevdp/PythonDataScienceHandbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors ##\n",
    "\n",
    "In pytorch, tensors are the core data structure. They are used to encode the models' inputs, outputs, and parameters. Tensors are optimized to do all the operations related to training and fast arithmetic calculation. Tensors are very similar to Numpy arrays in usage and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.BFloat16Tensor\n",
       "torch.BoolTensor\n",
       "torch.ByteTensor\n",
       "torch.CharTensor\n",
       "torch.DoubleTensor\n",
       "torch.FloatTensor\n",
       "torch.HalfTensor\n",
       "torch.IntTensor\n",
       "torch.LongTensor\n",
       "torch.ShortTensor\n",
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# All possible tensor types\n",
    "torch.*Tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super(Model, self).__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool\n",
       "\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\anaconda3\\envs\\pdl\\lib\\site-packages\\torch\\nn\\modules\\module.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     Identity, Linear, Bilinear, _ConvNd, Threshold, ReLU, RReLU, Hardtanh, Sigmoid, Hardsigmoid, ...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get summary information about the different classes (if possible and exist)\n",
    "torch.nn.Module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full definition of the class\n",
    "torch.nn.Module??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a Tensor ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4]]\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize tensor directly from native data in python\n",
    "\n",
    "data = [[1, 2],[3, 4]]\n",
    "data_in_tensor = torch.tensor(data)\n",
    "print(data)\n",
    "print(data_in_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# From numpy array\n",
    "\n",
    "np_array = np.array(data)\n",
    "nparray_in_tensor = torch.from_numpy(np_array)\n",
    "print(np_array)\n",
    "print(nparray_in_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9197, 0.2062],\n",
      "        [0.7578, 0.3463]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From another tensor (retaining shape and datatype)\n",
    "\n",
    "# When using \"..._like\" you retain shape\n",
    "x_ones = torch.ones_like(data_in_tensor)\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "# When using \"..._like\" you retain shape. You can override the datatype by setting dtype\n",
    "x_rand = torch.rand_like(data_in_tensor, dtype=torch.float)\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.9148, 0.6755, 0.7758],\n",
      "        [0.3699, 0.0501, 0.0677]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "torch.float32\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Ones Tensor: \n",
      " tensor([[1, 1, 1],\n",
      "        [1, 1, 1]]) \n",
      "\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# With random or constant values\n",
    "\n",
    "shape = (2,3,) # Used as a tuple of tensor dimensions\n",
    "# Float tensors by default\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape) \n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(ones_tensor.dtype)\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n",
    "\n",
    "ones_tensor = torch.ones(shape, dtype=torch.long)\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(ones_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n",
      "Num elements in tensor: 12\n"
     ]
    }
   ],
   "source": [
    "## Attributes\n",
    "\n",
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "print(f\"Num elements in tensor: {tensor.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor(1.)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Scatter lets you add a value in a specific position\n",
    "# Gather lets you get a value from a specific position\n",
    "\n",
    "tensor = torch.zeros(10, dtype=torch.float)\n",
    "print(tensor)\n",
    "#dim, position, value\n",
    "tensor.scatter_(0, torch.tensor(1), value=1)\n",
    "print(tensor)\n",
    "\n",
    "# tensor to gather, dim, position\n",
    "b = torch.gather(tensor, 0, torch.tensor(1))\n",
    "print(b)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on tensors ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row:  tensor([1., 1., 1., 1.])\n",
      "First column:  tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Standard numpy-like indexing and slicing:\n",
    "\n",
    "tensor = torch.ones(4, 4)\n",
    "print('First row: ',tensor[0])\n",
    "print('First column: ', tensor[:, 0])\n",
    "print('Last column:', tensor[..., -1])\n",
    "\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic tensor: \n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "torch.Size([4, 4])\n",
      "concat in rows: \n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "torch.Size([12, 4])\n",
      "concat in columns: \n",
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
      "torch.Size([4, 12])\n",
      "tensor([[[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]],\n",
      "\n",
      "        [[1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.],\n",
      "         [1., 0., 1., 1.]]])\n",
      "torch.Size([3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "#Joining tensors You can use torch.cat to concatenate a sequence of tensors \n",
    "#along a given dimension.\n",
    "\n",
    "print(\"basic tensor: \")\n",
    "print(tensor)\n",
    "print(tensor.shape)\n",
    "\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=0)\n",
    "print(\"concat in rows: \")\n",
    "print(t1)\n",
    "print(t1.shape)\n",
    "\n",
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(\"concat in columns: \")\n",
    "print(t1)\n",
    "print(t1.shape)\n",
    "\n",
    "##Adds a new dim also\n",
    "t1 = torch.stack([tensor, tensor, tensor], dim = 0)\n",
    "print(t1)\n",
    "print(t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "y1:\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "y2:\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "y3:\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#Arithmetic operations\n",
    "\n",
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# tensor.T = 0-D and 1-D tensors are returned as is.\n",
    "# When input is a 2-D tensor this is equivalent to transpose(input, 0, 1).\n",
    "print(tensor.T)\n",
    "print(tensor)\n",
    "y1 = tensor @ tensor.T\n",
    "print(\"y1:\")\n",
    "print(y1)\n",
    "\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "print(\"y2:\")\n",
    "print(y2)\n",
    "\n",
    "y3 = torch.rand_like(tensor)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "print(\"y3:\")\n",
    "print(y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)\n",
    "print(z1)\n",
    "\n",
    "z1 = tensor * tensor.T\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.)\n",
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "#Single-element tensors If you have a one-element tensor, \n",
    "#for example by aggregating all values of a tensor into one value, \n",
    "#you can convert it to a Python numerical value using item():\n",
    "\n",
    "# tensor.sum() sum all the values in tensor (reshaping the tensor as well)\n",
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]]) \n",
      "\n",
      "tensor([[11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.],\n",
      "        [11., 10., 11., 11.]])\n"
     ]
    }
   ],
   "source": [
    "#In-place operations Operations that store the result into the operand are \n",
    "#called in-place. \n",
    "#They are denoted by a _ suffix. For example: x.copy_(y), x.t_(), will change x.\n",
    "\n",
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bridge with Numpy ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Tensor to NumPy array\n",
    "\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy() #this is a reference\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "#A change in the tensor reflects in the NumPy array.\n",
    "\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NumPy array to Tensor\n",
    "\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n) #This is a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the shape of a tensor ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1]])\n",
      "torch.Size([4, 5])\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [0, 0, 0, 0],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]])\n",
      "torch.Size([5, 4])\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [0, 0, 0, 0],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]])\n",
      "torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 5, dtype=torch.long)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n",
    "print(tensor.shape)\n",
    "\n",
    "# With permute we select the dimensions we want to exchange\n",
    "permuted_tensor = tensor.permute(1, 0)\n",
    "print(permuted_tensor)\n",
    "print(permuted_tensor.shape)\n",
    "\n",
    "# permute is equivalent to permute\n",
    "transposed_tensor = torch.transpose(tensor, 0, 1)\n",
    "print(transposed_tensor)\n",
    "print(transposed_tensor.shape)\n",
    "# output = torch.reshape(tensor, ((seq_length) * curr_batch_size, -1))\n",
    "# output = tensor.view(-1, trg_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1]])\n",
      "torch.Size([4, 5])\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1])\n",
      "torch.Size([20])\n",
      "tensor([[[1, 0, 1, 1, 1, 1, 0, 1, 1, 1]],\n",
      "\n",
      "        [[1, 0, 1, 1, 1, 1, 0, 1, 1, 1]]])\n",
      "torch.Size([2, 1, 10])\n",
      "tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1])\n",
      "tensor([[1, 0, 0, 1, 1],\n",
      "        [1, 0, 0, 1, 1],\n",
      "        [1, 0, 0, 1, 1],\n",
      "        [1, 0, 0, 1, 1]])\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1])\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 5, dtype=torch.long)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n",
    "print(tensor.shape)\n",
    "\n",
    "# With view we reshape the tensor. We use -1 when we just want the dimension to have the \"rest\"\n",
    "# it is used to change the tensor to the specified dimensions\n",
    "# using view(-1) like this \"flatten\" the tensor\n",
    "view_applied_tensor = tensor.view(-1)\n",
    "print(view_applied_tensor)\n",
    "print(view_applied_tensor.shape)\n",
    "\n",
    "# Get another view of the tensor with 3 dims\n",
    "view_applied_tensor = tensor.view(2, 1, -1)\n",
    "print(view_applied_tensor)\n",
    "print(view_applied_tensor.shape)\n",
    "\n",
    "# Another peculiarity with view is that the data is shared with the original tensor\n",
    "view_applied_tensor = tensor.view(-1)\n",
    "print(view_applied_tensor)\n",
    "tensor[:,2] = 0\n",
    "print(tensor)\n",
    "print(view_applied_tensor)\n",
    "\n",
    "\n",
    "# Another approach is to use torch.reshape(tensor, set of dimensions)\n",
    "# It might return a tensor that does not share the data with the original tensor\n",
    "reshaped_tensor = torch.reshape(tensor, ([-1]))\n",
    "print(reshaped_tensor)\n",
    "print(reshaped_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1]])\n",
      "torch.Size([4, 5])\n",
      "tensor([[[1, 0, 1, 1, 1],\n",
      "         [1, 0, 1, 1, 1],\n",
      "         [1, 0, 1, 1, 1],\n",
      "         [1, 0, 1, 1, 1]]])\n",
      "torch.Size([1, 4, 5])\n",
      "tensor([[1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 0, 1, 1, 1]])\n",
      "torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 5, dtype=torch.long)\n",
    "tensor[:,1] = 0\n",
    "print(tensor)\n",
    "print(tensor.shape)\n",
    "\n",
    "# unsqueeze and squeeze are two other common operations\n",
    "# unsqueeze adds a dimension of size 1 at an specific dimension position (cannot be higher)\n",
    "# (tensor, dim)\n",
    "unsqueezed_tensor = torch.unsqueeze(tensor, 0)\n",
    "print(unsqueezed_tensor)\n",
    "print(unsqueezed_tensor.shape)\n",
    "\n",
    "# All dimensions of 1 are removed! (tensor, dim)\n",
    "squeezed_tensor = torch.squeeze(unsqueezed_tensor, 0)\n",
    "print(squeezed_tensor)\n",
    "print(squeezed_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "You should check this, and try/play with all the possible operations with tensors!! https://pytorch.org/docs/stable/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
