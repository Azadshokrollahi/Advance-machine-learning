{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Neural Network #\n",
    "\n",
    "Neural networks comprise of layers/modules that perform operations on data. The torch.nn namespace provides all the building blocks you need to build your own neural network. Every module in PyTorch subclasses the nn.Module. A neural network is a module itself that consists of other modules (layers). This nested structure allows for building and managing complex architectures easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. \n",
    "# Every nn.Module subclass implements the operations on input data in the forward method.\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_width, input_height, label_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten() # Used to convert the 2D input into 1D (28x28 to 784)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_width*input_height, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, label_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.softmax_result = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        self.softmax_result = self.softmax(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One by one the modules ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3,28,28) # 3 is used to exemplify a mini-batch\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "# We initialize the nn.Flatten layer to convert each 2D 28x28 \n",
    "# image into a contiguous array of 784 pixel values ( the minibatch dimension (at dim=0) is maintained).\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20) # Linear transformation\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
    "\n",
    "In this model, we use nn.ReLU between our linear layers, but there’s other activations to introduce non-linearity in your model such as LeakyReLU, Sigmoid, TANH, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 1.7573e-02,  4.1694e-01, -6.7020e-02, -9.7063e-03, -6.4914e-02,\n",
      "          3.9334e-01,  4.5783e-02,  2.9296e-01,  5.6483e-01, -2.8356e-01,\n",
      "          8.3396e-02,  5.7822e-01,  2.4769e-01, -6.5515e-01, -1.9853e-01,\n",
      "          1.0206e+00,  6.0045e-02, -7.1862e-04, -2.0854e-01, -9.5183e-02],\n",
      "        [ 3.0395e-01,  5.8897e-01, -8.0327e-02, -7.1482e-03, -3.9476e-01,\n",
      "          2.1905e-01, -9.7230e-02,  1.9060e-01,  9.7130e-02, -2.5420e-02,\n",
      "         -8.3685e-02,  8.0775e-01,  4.0999e-01, -8.6468e-01, -2.2341e-01,\n",
      "          6.3974e-01,  3.4347e-01,  3.7644e-01,  1.3201e-01, -1.3214e-01],\n",
      "        [ 9.7511e-02,  5.5372e-01,  2.7525e-01,  3.4343e-02,  5.8041e-02,\n",
      "          1.5501e-01, -3.9706e-01, -1.5643e-01,  2.0734e-01, -2.7891e-01,\n",
      "         -6.4181e-02,  6.8991e-01,  3.9327e-01, -5.5614e-01,  2.0688e-01,\n",
      "          2.7230e-01, -1.3768e-01,  4.9278e-02, -4.7706e-02,  9.8192e-02]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0176, 0.4169, 0.0000, 0.0000, 0.0000, 0.3933, 0.0458, 0.2930, 0.5648,\n",
      "         0.0000, 0.0834, 0.5782, 0.2477, 0.0000, 0.0000, 1.0206, 0.0600, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.3040, 0.5890, 0.0000, 0.0000, 0.0000, 0.2190, 0.0000, 0.1906, 0.0971,\n",
      "         0.0000, 0.0000, 0.8077, 0.4100, 0.0000, 0.0000, 0.6397, 0.3435, 0.3764,\n",
      "         0.1320, 0.0000],\n",
      "        [0.0975, 0.5537, 0.2752, 0.0343, 0.0580, 0.1550, 0.0000, 0.0000, 0.2073,\n",
      "         0.0000, 0.0000, 0.6899, 0.3933, 0.0000, 0.2069, 0.2723, 0.0000, 0.0493,\n",
      "         0.0000, 0.0982]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "nn.Sequential is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network like seq_modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1572,  0.3569,  0.1734, -0.2430, -0.1336,  0.0281, -0.0052,  0.3756,\n",
      "          0.3543, -0.1139],\n",
      "        [ 0.0756,  0.2918,  0.0968, -0.2669, -0.1232, -0.0526, -0.0187,  0.4116,\n",
      "          0.3053, -0.0704],\n",
      "        [ 0.1789,  0.1703,  0.1015, -0.2708, -0.2091, -0.0394, -0.1522,  0.3546,\n",
      "          0.4009, -0.0705]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3,28,28)\n",
    "logits = seq_modules(input_image)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Finally, we use the nn.softmax, which scale the logits values to [0, 1] from [-infty. infty], representing the models predicted probabilties.\n",
    "\n",
    "`dim` indicates which dimension should softmax be applied (dim=1 so we don't mess with the minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1041, 0.1271, 0.1058, 0.0697, 0.0778, 0.0915, 0.0885, 0.1295, 0.1267,\n",
      "         0.0794],\n",
      "        [0.0990, 0.1229, 0.1011, 0.0703, 0.0811, 0.0871, 0.0901, 0.1385, 0.1245,\n",
      "         0.0855],\n",
      "        [0.1114, 0.1105, 0.1031, 0.0711, 0.0756, 0.0896, 0.0800, 0.1328, 0.1391,\n",
      "         0.0868]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "print(pred_probab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(28, 28, 10).to(device)\n",
    "print(model) # You can see the networks structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0010,  0.0033,  0.0177,  ...,  0.0127,  0.0292, -0.0290],\n",
      "        [-0.0174,  0.0110, -0.0302,  ..., -0.0294,  0.0299,  0.0127]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([0.0060, 0.0191], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0298,  0.0357,  0.0310,  ...,  0.0256, -0.0139, -0.0340],\n",
      "        [ 0.0189,  0.0280, -0.0174,  ..., -0.0019, -0.0233,  0.0098]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0029, -0.0419], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0071,  0.0005, -0.0055,  ..., -0.0181, -0.0421,  0.0425],\n",
      "        [-0.0114, -0.0114, -0.0278,  ...,  0.0169,  0.0062, -0.0368]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0094,  0.0248], device='cuda:0', grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0984, 0.1063, 0.0955, 0.0998, 0.1063, 0.0993, 0.0955, 0.0970, 0.0955,\n",
      "         0.1065]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Predicted class: tensor([9], device='cuda:0')\n",
      "tensor([[0.0984, 0.1063, 0.0955, 0.0998, 0.1063, 0.0993, 0.0955, 0.0970, 0.0955,\n",
      "         0.1065]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "Predicted class: tensor([9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(pred_probab)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n",
    "\n",
    "print(model.softmax_result)\n",
    "pred_probab = model.softmax_result.argmax(1)  # Done directly in the network?\n",
    "print(f\"Predicted class: {y_pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
